{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Parquet data in 3.67 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag, when, unix_timestamp, lit\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName(\"lvb-spark\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.0') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read from the filtered Parquet file\n",
    "start_time = time.time()\n",
    "df = spark.read.parquet(\"data/filtered_01.parquet\")\n",
    "load_time = time.time() - start_time\n",
    "print(f\"Loaded Parquet data in {load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lag, when, unix_timestamp, lit\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Added Delay\n",
    "window_spec = Window.partitionBy(\"tripId\").orderBy(\"plannedWhen\")\n",
    "\n",
    "enriched_df = df.withColumn(\"prev_delay\", lag(\"delay\").over(window_spec)) \\\n",
    "    .withColumn(\"added_delay\", when(col(\"prev_delay\").isNotNull(), col(\"delay\") - col(\"prev_delay\")).otherwise(lit(0))) \\\n",
    "    .drop(\"prev_delay\")\n",
    "\n",
    "enriched_df.cache()\n",
    "\n",
    "enriched_df.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+-----+-----------------+-----------+-------------------+------+--------------------+----------+-----------+---------+\n",
      "|__v|                 _id| crawlDate|delay|        direction|     lineId|        plannedWhen|stopId|              tripId|      when|added_delay|stop_type|\n",
      "+---+--------------------+----------+-----+-----------------+-----------+-------------------+------+--------------------+----------+-----------+---------+\n",
      "|  0|{65802eab5147b56c...|2023-12-19|    0|           Thekla|5-naslvb-82|2023-12-19 00:00:00|958178|1|1002862|1|81|18...|2023-12-19|          0|    start|\n",
      "|  0|{65802a782a63c003...|2023-12-19|    0|           Thekla|5-naslvb-82|2023-12-19 00:00:00|962670|1|1002862|1|81|18...|2023-12-19|          0|      end|\n",
      "|  0|{658029c5fe3de42a...|2023-12-19|   60|           Thekla|5-naslvb-82|2023-12-19 00:00:00|964552|1|1002862|1|81|18...|2023-12-19|         60|      end|\n",
      "|  0|{65802c8f4afec018...|2023-12-19|    0|           Thekla|5-naslvb-82|2023-12-19 00:00:00|958311|1|1002862|1|81|18...|2023-12-19|        -60|      end|\n",
      "|  0|{658029c5fe3de42a...|2023-12-19|   60|           Thekla|5-naslvb-82|2023-12-19 00:00:00|963958|1|1002862|1|81|18...|2023-12-19|         60|      end|\n",
      "|  0|{65802df77d934edb...|2023-12-19|    0|           Thekla|5-naslvb-82|2023-12-19 00:00:00|957178|1|1002862|1|81|18...|2023-12-19|        -60|      end|\n",
      "|  0|{65802be1ba163832...|2023-12-19|    0|           Thekla|5-naslvb-82|2023-12-19 00:00:00|957499|1|1002862|1|81|18...|2023-12-19|          0|      end|\n",
      "|  0|{65802eae5147b56c...|2023-12-19|    0|           Thekla|5-naslvb-82|2023-12-19 00:00:00|955799|1|1002862|1|81|18...|2023-12-19|          0|      end|\n",
      "|  0|{65802be1ba163832...|2023-12-19|    0|           Thekla|5-naslvb-82|2023-12-19 00:00:00|955815|1|1002862|1|81|18...|2023-12-19|          0|      end|\n",
      "|  0|{65802c924afec018...|2023-12-19|    0|           Thekla|5-naslvb-82|2023-12-19 00:00:00|956123|1|1002862|1|81|18...|2023-12-19|          0|      end|\n",
      "|  0|{65802d4c0ebe60d3...|2023-12-19|    0|           Thekla|5-naslvb-82|2023-12-19 00:00:00|954844|1|1002862|1|81|18...|2023-12-19|          0|      end|\n",
      "|  0|{65802be0ba163832...|2023-12-19|    0|           Thekla|5-naslvb-82|2023-12-19 00:00:00|955204|1|1002862|1|81|18...|2023-12-19|          0|      end|\n",
      "|  0|{65802df97d934edb...|2023-12-19|    0|           Thekla|5-naslvb-82|2023-12-19 00:00:00|957545|1|1002862|1|81|18...|2023-12-19|          0|      end|\n",
      "|  0|{65802df97d934edb...|2023-12-19|    0|           Thekla|5-naslvb-82|2023-12-19 00:00:00|957861|1|1002862|1|81|18...|2023-12-19|          0|      end|\n",
      "|  0|{6580290e05115efb...|2023-12-19|   60|           Thekla|5-naslvb-82|2023-12-19 00:00:00|961862|1|1002862|1|81|18...|2023-12-19|         60|      end|\n",
      "|  0|{657ffd74252aca76...|2023-12-19|    0|     Hauptbahnhof|8-naslvt-10|2023-12-19 00:00:00|955189|1|1005405|1|81|18...|2023-12-19|          0|    start|\n",
      "|  0|{657fff91aeb10820...|2023-12-19|   60|     Hauptbahnhof|8-naslvt-10|2023-12-19 00:00:00|955254|1|1005405|1|81|18...|2023-12-19|         60|      end|\n",
      "|  0|{657ffede30db7816...|2023-12-19|   60|     Hauptbahnhof|8-naslvt-10|2023-12-19 00:00:00|955794|1|1005405|1|81|18...|2023-12-19|          0|      end|\n",
      "|  0|{65800316ba01e232...|2023-12-19|   60|Paunsdorf, Strbf.|8-naslvt-10|2023-12-19 00:00:00|955803|1|1005405|1|81|18...|2023-12-19|          0|      end|\n",
      "|  0|{658005304f2d72a1...|2023-12-19|   60|Paunsdorf, Strbf.|8-naslvt-10|2023-12-19 00:00:00|956058|1|1005405|1|81|18...|2023-12-19|          0|      end|\n",
      "+---+--------------------+----------+-----+-----------------+-----------+-------------------+------+--------------------+----------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, when\n",
    "\n",
    "# Stop Type\n",
    "window_spec_trip = Window.partitionBy(\"tripId\")\n",
    "\n",
    "df_with_stop_type = enriched_df.withColumn(\n",
    "    \"stop_type\",\n",
    "    when(\n",
    "        col(\"stopId\") == first(\"stopId\").over(window_spec_trip),\n",
    "        \"start\"\n",
    "    ).when(\n",
    "        col(\"stopId\") == last(\"stopId\").over(window_spec_trip),\n",
    "        \"end\"\n",
    "    ).otherwise(\"pass\")\n",
    ")\n",
    "\n",
    "enriched_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:======================>                                 (80 + 8) / 200]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save to Parquet\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43menriched_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/enriched_01.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData saved to Parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/readwriter.py:885\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m--> 885\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1308\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1310\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py:475\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    476\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Save to Parquet\n",
    "enriched_df.write.mode(\"overwrite\").parquet(\"data/enriched_01.parquet\")\n",
    "print(\"Data saved to Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Parquet\n",
    "start_time = time.time()\n",
    "parquet_df = spark.read.parquet(\"data/enriched_01.parquet\")\n",
    "load_time = time.time() - start_time\n",
    "print(f\"Loaded Parquet data in {load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Get a random tripId\n",
    "random_trip = parquet_df.select(\"tripId\").distinct().orderBy(rand()).limit(1).collect()[0][\"tripId\"]\n",
    "# Filter the dataframe for the selected trip and order by plannedWhen\n",
    "trip_stops = parquet_df.filter(col(\"tripId\") == random_trip) \\\n",
    "                       .select(\"stopId\", \"plannedWhen\", \"when\", \"delay\", \"added_delay\") \\\n",
    "                       .orderBy(\"plannedWhen\")\n",
    "\n",
    "print(f\"Stops for trip {random_trip}:\")\n",
    "trip_stops.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
