{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out irrelevant data\n",
    "\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max as spark_max, row_number, to_timestamp, date_add\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"lvb-spark\") \\\n",
    "    .config('spark.master', 'local') \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.0') \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load departures data from MongoDB\n",
    "df = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", \"mongodb://mongo:27017/\") \\\n",
    "    .option(\"database\", \"lvb\") \\\n",
    "    .option(\"collection\", \"departures\") \\\n",
    "    .load()\n",
    "\n",
    "## METRICS\n",
    "total_rows = df.count()\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "## TIMEZONE\n",
    "# Convert plannedWhen, crawlDate, and when from string to timestamp\n",
    "df = df.withColumn(\"plannedWhen\", to_timestamp(col(\"plannedWhen\")))\n",
    "df = df.withColumn(\"crawlDate\", to_timestamp(col(\"crawlDate\")))\n",
    "df = df.withColumn(\"when\", to_timestamp(col(\"when\")))\n",
    "\n",
    "# Shift time from UTC to UTC+1\n",
    "df = df.withColumn(\"plannedWhen\", expr(\"plannedWhen + INTERVAL 1 HOUR\"))\n",
    "df = df.withColumn(\"crawlDate\", expr(\"crawlDate + INTERVAL 1 HOUR\"))\n",
    "df = df.withColumn(\"when\", expr(\"when + INTERVAL 1 HOUR\"))\n",
    "\n",
    "## DATE FILTERING\n",
    "# Define the date range\n",
    "start_date = \"2023-12-08 00:00:00\"\n",
    "end_date = \"2024-01-14 23:59:59\"\n",
    "\n",
    "# Filter by date range\n",
    "df = df.filter((col(\"plannedWhen\") >= start_date) & (col(\"plannedWhen\") <= end_date))\n",
    "\n",
    "## DEDUPLICATION\n",
    "window_spec = Window.partitionBy(\"tripId\", \"stopId\", \"plannedWhen\").orderBy(col(\"crawlDate\").desc())\n",
    "\n",
    "# Add row numbers within each window\n",
    "df = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "# Keep only the first row (latest crawlDate) for each window\n",
    "filtered_df = df.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "# Count the number of rows before and after deduplication\n",
    "rows_before = df.count()\n",
    "rows_after = filtered_df.count()\n",
    "removed_rows = rows_before - rows_after\n",
    "\n",
    "print(f\"Removed {removed_rows} duplicate rows. {rows_after} rows remain after deduplication.\")\n",
    "\n",
    "## STOPS\n",
    "df = filtered_df\n",
    "\n",
    "# Load relevant stops\n",
    "with open('data/stops_fromRelevantLines.json', 'r') as f:\n",
    "    relevant_stops = json.load(f)\n",
    "relevant_stop_ids = list(relevant_stops.keys())\n",
    "\n",
    "# Filter departures by relevant stops\n",
    "filtered_df = df.filter(col(\"stopId\").isin(relevant_stop_ids))\n",
    "\n",
    "# Count filtered stops\n",
    "total_stops = df.select(\"stopId\").distinct().count()\n",
    "remaining_stops = filtered_df.select(\"stopId\").distinct().count()\n",
    "filtered_stops = total_stops - remaining_stops\n",
    "\n",
    "print(f\"Filtered out {filtered_stops} stops. {remaining_stops} stops remain.\")\n",
    "\n",
    "## LINES\n",
    "df = filtered_df\n",
    "\n",
    "# Load relevant lines\n",
    "with open('data/lines_with_stops.json', 'r') as f:\n",
    "    relevant_lines = json.load(f)\n",
    "relevant_line_ids = list(relevant_lines.keys())\n",
    "\n",
    "total_lines = df.select(\"lineId\").distinct().count()\n",
    "\n",
    "# filter by relevant lines\n",
    "filtered_df = df.filter(col(\"lineId\").isin(relevant_line_ids))\n",
    "\n",
    "# Count filtered lines\n",
    "remaining_lines = filtered_df.select(\"lineId\").distinct().count()\n",
    "filtered_lines = total_lines - remaining_lines\n",
    "\n",
    "print(f\"Filtered out {filtered_lines} lines. {remaining_lines} lines remain.\")\n",
    "\n",
    "## HANDLE NULLS\n",
    "filtered_df = filtered_df.fillna({'delay': 0})\n",
    "\n",
    "\n",
    "## METRICS\n",
    "filtered_rows = filtered_df.count()\n",
    "print(f\"Total Filter Stats: Filtered out {total_rows - filtered_rows} rows. {filtered_rows} rows remain.\")\n",
    "\n",
    "# Output as Parquet\n",
    "output_path = \"data/filtered_01.parquet\"\n",
    "filtered_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"Filtered data saved to {output_path}\")\n",
    "\n",
    "# Show sample of the filtered data\n",
    "filtered_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the saved Parquet file\n",
    "parquet_df = spark.read.parquet(output_path)\n",
    "\n",
    "# Show sample of the filtered data from the Parquet file\n",
    "print(\"Sample of data from the Parquet file:\")\n",
    "parquet_df.show(5)\n",
    "\n",
    "print(\"Schema of the Parquet file:\")\n",
    "parquet_df.printSchema()\n",
    "\n",
    "# Optional: Get some basic statistics\n",
    "print(\"Summary statistics:\")\n",
    "parquet_df.describe().show()\n",
    "# Count total number of rows\n",
    "total_rows = parquet_df.count()\n",
    "mongo_count = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", \"mongodb://mongo:27017/\") \\\n",
    "    .option(\"database\", \"lvb\") \\\n",
    "    .option(\"collection\", \"departures\") \\\n",
    "    .load().count()\n",
    "\n",
    "# Calculate the percentage of reduced data rows\n",
    "reduced_percentage = ((mongo_count - total_rows) / mongo_count) * 100 if mongo_count > 0 else 0\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "print(f\"Percentage of reduced data rows: {reduced_percentage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
