{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1956332 duplicate rows. 4840435 rows remain after deduplication.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 134 stops. 659 stops remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 0 lines. 52 lines remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Filter Stats: Filtered out 3555799 rows. 4120672 rows remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to data/filtered_01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 123:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+-----+--------------------+-----------+-------------------+------+--------------------+----------+\n",
      "|__v|                 _id| crawlDate|delay|           direction|     lineId|        plannedWhen|stopId|              tripId|      when|\n",
      "+---+--------------------+----------+-----+--------------------+-----------+-------------------+------+--------------------+----------+\n",
      "|  0|{657e724cd9f42ed9...|2023-12-18|  120|          Stötteritz| 8-naslvt-4|2023-12-18 00:00:00|958145|1|1000039|0|81|17...|2023-12-18|\n",
      "|  0|{657bf8c9da53b8a4...|2023-12-16|  120|BMW Zentralgebäud...|5-naslvb-82|2023-12-16 00:00:00|955799|1|1000092|0|81|15...|2023-12-16|\n",
      "|  0|{657bfb98e996a6f5...|2023-12-16|  120|BMW Zentralgebäud...|5-naslvb-82|2023-12-16 00:00:00|955815|1|1000092|0|81|15...|2023-12-16|\n",
      "|  0|{657bfaec1545b917...|2023-12-16|  120|BMW Zentralgebäud...|5-naslvb-82|2023-12-16 00:00:00|956123|1|1000092|0|81|15...|2023-12-16|\n",
      "|  0|{657bfb99e996a6f5...|2023-12-16|  180|BMW Zentralgebäud...|5-naslvb-82|2023-12-16 00:00:00|957499|1|1000092|0|81|15...|2023-12-16|\n",
      "+---+--------------------+----------+-----+--------------------+-----------+-------------------+------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter out irrelevant data\n",
    "\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max as spark_max, row_number, to_timestamp, date_add\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"lvb-spark\") \\\n",
    "    .config('spark.master', 'local') \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.0') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load departures data from MongoDB\n",
    "df = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", \"mongodb://mongo:27017/\") \\\n",
    "    .option(\"database\", \"lvb\") \\\n",
    "    .option(\"collection\", \"departures\") \\\n",
    "    .load()\n",
    "\n",
    "## METRICS\n",
    "total_rows = df.count()\n",
    "\n",
    "## TIMEZONE\n",
    "# Shift time from UTC to UTC+1\n",
    "df = df.withColumn(\"plannedWhen\", date_add(col(\"plannedWhen\"), 1))\n",
    "df = df.withColumn(\"crawlDate\", date_add(col(\"crawlDate\"), 1))\n",
    "df = df.withColumn(\"when\", date_add(col(\"when\"), 1))\n",
    "\n",
    "## DATE FILTERING\n",
    "# Define the date range\n",
    "start_date = \"2023-12-08\"\n",
    "end_date = \"2024-01-14\"\n",
    "\n",
    "# Convert plannedWhen to timestamp and filter by date range\n",
    "df = df.withColumn(\"plannedWhen\", to_timestamp(col(\"plannedWhen\")))\n",
    "df = df.filter((col(\"plannedWhen\") >= start_date) & (col(\"plannedWhen\") <= end_date))\n",
    "\n",
    "## DEDUPLICATION\n",
    "window_spec = Window.partitionBy(\"tripId\", \"stopId\", \"plannedWhen\").orderBy(col(\"crawlDate\").desc())\n",
    "\n",
    "# Add row numbers within each window\n",
    "df = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "# Keep only the first row (latest crawlDate) for each window\n",
    "filtered_df = df.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "# Count the number of rows before and after deduplication\n",
    "rows_before = df.count()\n",
    "rows_after = filtered_df.count()\n",
    "removed_rows = rows_before - rows_after\n",
    "\n",
    "print(f\"Removed {removed_rows} duplicate rows. {rows_after} rows remain after deduplication.\")\n",
    "\n",
    "## STOPS\n",
    "df = filtered_df\n",
    "\n",
    "# Load relevant stops\n",
    "with open('data/stops_fromRelevantLines.json', 'r') as f:\n",
    "    relevant_stops = json.load(f)\n",
    "relevant_stop_ids = list(relevant_stops.keys())\n",
    "\n",
    "# Filter departures by relevant stops\n",
    "filtered_df = df.filter(col(\"stopId\").isin(relevant_stop_ids))\n",
    "\n",
    "# Count filtered stops\n",
    "total_stops = df.select(\"stopId\").distinct().count()\n",
    "remaining_stops = filtered_df.select(\"stopId\").distinct().count()\n",
    "filtered_stops = total_stops - remaining_stops\n",
    "\n",
    "print(f\"Filtered out {filtered_stops} stops. {remaining_stops} stops remain.\")\n",
    "\n",
    "## LINES\n",
    "df = filtered_df\n",
    "\n",
    "# Load relevant lines\n",
    "with open('data/lines_with_stops.json', 'r') as f:\n",
    "    relevant_lines = json.load(f)\n",
    "relevant_line_ids = list(relevant_lines.keys())\n",
    "\n",
    "total_lines = df.select(\"lineId\").distinct().count()\n",
    "\n",
    "# filter by relevant lines\n",
    "filtered_df = df.filter(col(\"lineId\").isin(relevant_line_ids))\n",
    "\n",
    "# Count filtered lines\n",
    "remaining_lines = filtered_df.select(\"lineId\").distinct().count()\n",
    "filtered_lines = total_lines - remaining_lines\n",
    "\n",
    "print(f\"Filtered out {filtered_lines} lines. {remaining_lines} lines remain.\")\n",
    "\n",
    "## METRICS\n",
    "filtered_rows = filtered_df.count()\n",
    "print(f\"Total Filter Stats: Filtered out {total_rows - filtered_rows} rows. {filtered_rows} rows remain.\")\n",
    "\n",
    "# Output as Parquet\n",
    "output_path = \"data/filtered_01.parquet\"\n",
    "filtered_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"Filtered data saved to {output_path}\")\n",
    "\n",
    "# Show sample of the filtered data\n",
    "filtered_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the saved Parquet file\n",
    "parquet_df = spark.read.parquet(output_path)\n",
    "\n",
    "# Show sample of the filtered data from the Parquet file\n",
    "print(\"Sample of data from the Parquet file:\")\n",
    "parquet_df.show(5)\n",
    "\n",
    "print(\"Schema of the Parquet file:\")\n",
    "parquet_df.printSchema()\n",
    "\n",
    "# Optional: Get some basic statistics\n",
    "print(\"Summary statistics:\")\n",
    "parquet_df.describe().show()\n",
    "# Count total number of rows\n",
    "total_rows = parquet_df.count()\n",
    "mongo_count = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", \"mongodb://mongo:27017/\") \\\n",
    "    .option(\"database\", \"lvb\") \\\n",
    "    .option(\"collection\", \"departures\") \\\n",
    "    .load().count()\n",
    "\n",
    "# Calculate the percentage of reduced data rows\n",
    "reduced_percentage = ((mongo_count - total_rows) / mongo_count) * 100 if mongo_count > 0 else 0\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "print(f\"Percentage of reduced data rows: {reduced_percentage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
