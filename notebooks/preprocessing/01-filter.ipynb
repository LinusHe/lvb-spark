{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all lines and stops that are not relevant for the analysis\n",
    "\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"lvb-spark\") \\\n",
    "    .config('spark.master', 'local') \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.0') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load departures data from MongoDB\n",
    "df = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", \"mongodb://mongo:27017/\") \\\n",
    "    .option(\"database\", \"lvb\") \\\n",
    "    .option(\"collection\", \"departures\") \\\n",
    "    .load()\n",
    "\n",
    "# Load relevant stops\n",
    "with open('data/stops_fromRelevantLines.json', 'r') as f:\n",
    "    relevant_stops = json.load(f)\n",
    "\n",
    "# Load relevant lines\n",
    "with open('data/relevantLines_with_stops.json', 'r') as f:\n",
    "    relevant_lines = json.load(f)\n",
    "\n",
    "# Get list of relevant stop IDs and line IDs\n",
    "relevant_stop_ids = list(relevant_stops.keys())\n",
    "relevant_line_ids = list(relevant_lines.keys())\n",
    "\n",
    "# Filter departures by relevant stops\n",
    "filtered_df = df.filter(col(\"stopId\").isin(relevant_stop_ids))\n",
    "\n",
    "# Count filtered stops\n",
    "total_stops = df.select(\"stopId\").distinct().count()\n",
    "remaining_stops = filtered_df.select(\"stopId\").distinct().count()\n",
    "filtered_stops = total_stops - remaining_stops\n",
    "\n",
    "print(f\"Filtered out {filtered_stops} stops. {remaining_stops} stops remain.\")\n",
    "\n",
    "# Further filter by relevant lines\n",
    "final_df = filtered_df.filter(col(\"lineId\").isin(relevant_line_ids))\n",
    "\n",
    "# Count filtered lines\n",
    "total_lines = filtered_df.select(\"lineId\").distinct().count()\n",
    "remaining_lines = final_df.select(\"lineId\").distinct().count()\n",
    "filtered_lines = total_lines - remaining_lines\n",
    "\n",
    "print(f\"Filtered out {filtered_lines} lines. {remaining_lines} lines remain.\")\n",
    "\n",
    "# Output as Parquet\n",
    "output_path = \"data/filtered_departures.parquet\"\n",
    "final_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"Filtered data saved to {output_path}\")\n",
    "\n",
    "# Show sample of the filtered data\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of data from the Parquet file:\n",
      "+---+--------------------+--------------------+-----+-------------------+-----------+-------------------+------+--------------------+-------------------+\n",
      "|__v|                 _id|           crawlDate|delay|          direction|     lineId|        plannedWhen|stopId|              tripId|               when|\n",
      "+---+--------------------+--------------------+-----+-------------------+-----------+-------------------+------+--------------------+-------------------+\n",
      "|  0|{6583e768bf61bedc...|2023-12-21 07:21:...|   60|Technisches Rathaus|8-naslvt-12|2023-12-21 07:19:00|956126|1|1053955|11|81|2...|2023-12-21 07:20:00|\n",
      "|  0|{6583e768bf61bedc...|2023-12-21 07:21:...|   60|       Messegelände|8-naslvt-16|2023-12-21 07:19:00|956291|1|293004|11|81|21...|2023-12-21 07:20:00|\n",
      "|  0|{6583e768bf61bedc...|2023-12-21 07:21:...|    0|             Lößnig|8-naslvt-16|2023-12-21 07:20:00|956291|1|292994|10|81|21...|2023-12-21 07:20:00|\n",
      "|  0|{6583e768bf61bedc...|2023-12-21 07:21:...|  120|     Paunsdorf-Nord| 8-naslvt-8|2023-12-21 07:19:00|955524|1|293462|3|81|211...|2023-12-21 07:21:00|\n",
      "|  0|{6583e768bf61bedc...|2023-12-21 07:21:...|  120|         Stötteritz| 8-naslvt-4|2023-12-21 07:19:00|956291|1|294099|10|81|21...|2023-12-21 07:21:00|\n",
      "+---+--------------------+--------------------+-----+-------------------+-----------+-------------------+------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema of the Parquet file:\n",
      "root\n",
      " |-- __v: integer (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- crawlDate: timestamp (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- lineId: string (nullable = true)\n",
      " |-- plannedWhen: timestamp (nullable = true)\n",
      " |-- stopId: string (nullable = true)\n",
      " |-- tripId: string (nullable = true)\n",
      " |-- when: timestamp (nullable = true)\n",
      "\n",
      "Summary statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------------+--------------------+---------+------------------+--------------------+\n",
      "|summary|    __v|            delay|           direction|   lineId|            stopId|              tripId|\n",
      "+-------+-------+-----------------+--------------------+---------+------------------+--------------------+\n",
      "|  count|6923848|          6457767|             6923848|  6923848|           6923848|             6923848|\n",
      "|   mean|    0.0|94.40022224400478|                null|     null|1759677.5941112514|                null|\n",
      "| stddev|    0.0|313.0336486494021|                null|     null|2253638.2906197426|                null|\n",
      "|    min|      0|                0|<+> Borna  über E...|3-bb-re10|            130636|1|1000001|0|81|16...|\n",
      "|    max|      0|            30180|             fl.Text|      bus|            964749|1|999986|1|81|191...|\n",
      "+-------+-------+-----------------+--------------------+---------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the saved Parquet file\n",
    "parquet_df = spark.read.parquet(output_path)\n",
    "\n",
    "# Show sample of the filtered data from the Parquet file\n",
    "print(\"Sample of data from the Parquet file:\")\n",
    "parquet_df.show(5)\n",
    "\n",
    "# Optional: Display schema of the Parquet file\n",
    "print(\"Schema of the Parquet file:\")\n",
    "parquet_df.printSchema()\n",
    "\n",
    "# Optional: Get some basic statistics\n",
    "print(\"Summary statistics:\")\n",
    "parquet_df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:=====================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 6923848\n",
      "Percentage of reduced data rows: 9.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count total number of rows\n",
    "total_rows = parquet_df.count()\n",
    "mongo_count = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", \"mongodb://mongo:27017/\") \\\n",
    "    .option(\"database\", \"lvb\") \\\n",
    "    .option(\"collection\", \"departures\") \\\n",
    "    .load().count()\n",
    "\n",
    "# Calculate the percentage of reduced data rows\n",
    "reduced_percentage = ((mongo_count - total_rows) / mongo_count) * 100 if mongo_count > 0 else 0\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "print(f\"Percentage of reduced data rows: {reduced_percentage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
