{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all lines and stops that are not relevant for the analysis\n",
    "\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max as spark_max, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"lvb-spark\") \\\n",
    "    .config('spark.master', 'local') \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.0') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load departures data from MongoDB\n",
    "df = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", \"mongodb://mongo:27017/\") \\\n",
    "    .option(\"database\", \"lvb\") \\\n",
    "    .option(\"collection\", \"departures\") \\\n",
    "    .load()\n",
    "\n",
    "# Load relevant stops\n",
    "with open('data/stops_fromRelevantLines.json', 'r') as f:\n",
    "    relevant_stops = json.load(f)\n",
    "\n",
    "# Load relevant lines\n",
    "with open('data/lines_with_stops.json', 'r') as f:\n",
    "    relevant_lines = json.load(f)\n",
    "\n",
    "# Get list of relevant stop IDs and line IDs\n",
    "relevant_stop_ids = list(relevant_stops.keys())\n",
    "relevant_line_ids = list(relevant_lines.keys())\n",
    "\n",
    "# Filter departures by relevant stops\n",
    "filtered_df = df.filter(col(\"stopId\").isin(relevant_stop_ids))\n",
    "\n",
    "# Count filtered stops\n",
    "total_stops = df.select(\"stopId\").distinct().count()\n",
    "remaining_stops = filtered_df.select(\"stopId\").distinct().count()\n",
    "filtered_stops = total_stops - remaining_stops\n",
    "\n",
    "print(f\"Filtered out {filtered_stops} stops. {remaining_stops} stops remain.\")\n",
    "\n",
    "# Further filter by relevant lines\n",
    "final_df = filtered_df.filter(col(\"lineId\").isin(relevant_line_ids))\n",
    "\n",
    "# Count filtered lines\n",
    "total_lines = filtered_df.select(\"lineId\").distinct().count()\n",
    "remaining_lines = final_df.select(\"lineId\").distinct().count()\n",
    "filtered_lines = total_lines - remaining_lines\n",
    "\n",
    "print(f\"Filtered out {filtered_lines} lines. {remaining_lines} lines remain.\")\n",
    "\n",
    "# Output as Parquet\n",
    "output_path = \"data/filtered_01.parquet\"\n",
    "final_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"Filtered data saved to {output_path}\")\n",
    "\n",
    "# Show sample of the filtered data\n",
    "final_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "# Read the saved Parquet file\n",
    "parquet_df = spark.read.parquet(\"data/filtered_01.parquet\")\n",
    "\n",
    "# Deduplicate by tripId, stopId, and plannedWhen\n",
    "# Create a window specification\n",
    "window_spec = Window.partitionBy(\"tripId\", \"stopId\", \"plannedWhen\").orderBy(col(\"crawlDate\").desc())\n",
    "\n",
    "# Add row numbers within each window\n",
    "final_df_with_row_num = final_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "# Keep only the first row (latest crawlDate) for each window\n",
    "final_df = final_df_with_row_num.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "# Count the number of rows before and after deduplication\n",
    "rows_before = final_df_with_row_num.count()\n",
    "rows_after = final_df.count()\n",
    "removed_rows = rows_before - rows_after\n",
    "\n",
    "print(f\"Removed {removed_rows} duplicate rows. {rows_after} rows remain after deduplication.\")\n",
    "\n",
    "# Output as Parquet\n",
    "output_path = \"data/filtered_02.parquet\"\n",
    "final_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"Filtered data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the saved Parquet file\n",
    "parquet_df = spark.read.parquet(output_path)\n",
    "\n",
    "# Show sample of the filtered data from the Parquet file\n",
    "print(\"Sample of data from the Parquet file:\")\n",
    "parquet_df.show(5)\n",
    "\n",
    "# Optional: Display schema of the Parquet file\n",
    "print(\"Schema of the Parquet file:\")\n",
    "parquet_df.printSchema()\n",
    "\n",
    "# Optional: Get some basic statistics\n",
    "print(\"Summary statistics:\")\n",
    "parquet_df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total number of rows\n",
    "total_rows = parquet_df.count()\n",
    "mongo_count = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", \"mongodb://mongo:27017/\") \\\n",
    "    .option(\"database\", \"lvb\") \\\n",
    "    .option(\"collection\", \"departures\") \\\n",
    "    .load().count()\n",
    "\n",
    "# Calculate the percentage of reduced data rows\n",
    "reduced_percentage = ((mongo_count - total_rows) / mongo_count) * 100 if mongo_count > 0 else 0\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "print(f\"Percentage of reduced data rows: {reduced_percentage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
